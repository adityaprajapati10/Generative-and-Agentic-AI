## Introduction to RNN

Recurrent Neural Networks (RNNs) are a specialized type of neural network designed for **sequential data** like text, speech, and time series. They differ from traditional neural networks in their ability to retain information from previous time steps using internal memory.

In standard feedforward neural networks:
- Each input is processed independently.
- There's no memory of previous inputs.
- Only **forward propagation** and **backward propagation** are involved.

But this fails in scenarios where context matters ‚Äî for instance, predicting the next word in a sentence.

In neural network we learn forward propagation and then system learn through backward propagation  but we know every time system will be consider all of these input as a new input every time. system will not be able to retain any kind of information or knowledge from the previous input. there no linkage between a memory the previous input and next input. so basically it try to pass a new input every time and based on that new input it will try to just adjust the weights in between means try to change the relationship inside the network, this is what happed in backward propagation.

## How RNN Works

RNNs maintain a **hidden state** `h‚Çú`, which captures information from previous inputs. At each time step `t`, the network takes:
- The current input `x‚Çú`
- The previous hidden state `h‚Çú‚Çã‚ÇÅ`

and produces:
- A new hidden state `h‚Çú`
- An output `y‚Çú`

So the input is `[h‚Çú‚Çã‚ÇÅ, x‚Çú]`.

This way, RNNs can process sequences of arbitrary length and capture short-term dependencies in data.

## Limitation of RNN: Vanishing Gradient

RNNs struggle with **long-term dependencies** due to the **vanishing gradient problem**. As gradients are propagated backward through time, they become very small (or sometimes explode), making it difficult to learn from earlier time steps.

As a result:
- RNNs are good at remembering recent inputs.
- But they forget inputs from many time steps ago.

---

## LSTM: Long Short-Term Memory Networks

To overcome RNN's limitations, **LSTM** networks were introduced.

###  Key Strengths of LSTM:
- Maintains **long-term memory** via a memory cell.
- Uses **gates** to control what information to forget, remember, and output.
- Effectively handles **long-range dependencies**.

##  LSTM Architecture

LSTM includes the following components:

1. **Forget Gate** `f‚Çú`
2. **Input Gate** `i‚Çú`
3. **Candidate Memory** `CÃÉ‚Çú`
4. **Output Gate** `o‚Çú`
5. **Cell State** `C‚Çú`
6. **Hidden State** `h‚Çú`

All gates take as input:  
`[h‚Çú‚Çã‚ÇÅ, x‚Çú]`

### 1. Forget Gate

- Determines what information to discard from the cell state.

$$
f‚Çú = \sigma(h‚Çú‚Çã‚ÇÅ \cdot W‚Çï + X‚Çú \cdot W‚Çì + b)
$$

### 2. Input Gate and Candidate Memory

- **Input Gate** Decides **what new information to store**.
  $$
i‚Çú = \sigma(h‚Çú‚Çã‚ÇÅ \cdot W‚Çï·µ¢ + x‚Çú \cdot W‚Çì·µ¢ + b·µ¢)
$$

- **Candidate Memory** **new candidate values** to add to the cell state.

   $$
\tilde{C}_‚Çú = \tanh(h‚Çú‚Çã‚ÇÅ \cdot W‚Çïùöå + x‚Çú \cdot W‚Çìùöå + bùöå)
$$

- Update cell state: Updates the cell state with what we remembered and added.

$$
C‚Çú = f‚Çú \odot C‚Çú‚Çã‚ÇÅ + i‚Çú \odot \tilde{C}_‚Çú
$$

### 3. Output Gate 

 - Decides **what to output** as the hidden state.

   $$
o‚Çú = \sigma(h‚Çú‚Çã‚ÇÅ \cdot W‚Çï‚Çí + x‚Çú \cdot W‚Çì‚Çí + b‚Çí)
$$

$$
h‚Çú = o‚Çú \odot \tanh(C‚Çú)
$$



If we want to generate a prediction:

$$
y‚Çú = \text{softmax}(W \cdot h‚Çú + b)
$$

## Summary of Gates

| Gate         | Purpose               | Activation | Equation Style                         |
|--------------|------------------------|------------|-----------------------------------------|
| Forget Gate  | What to forget         | Sigmoid    | `f‚Çú = œÉ(h‚Çú‚Çã‚ÇÅ W‚Çï + x‚Çú W‚Çì + b)`           |
| Input Gate   | What to add            | Sigmoid    | `i‚Çú = œÉ(h‚Çú‚Çã‚ÇÅ W‚Çï·µ¢ + x‚Çú W‚Çì·µ¢ + b·µ¢)`        |
| Candidate    | Candidate memory       | Tanh       | `~C‚Çú = tanh(h‚Çú‚Çã‚ÇÅ W‚Çïùöå + x‚Çú W‚Çìùöå + bùöå)`     |
| Cell State   | Final memory value     | ‚Äî          | `C‚Çú = f‚Çú * C‚Çú‚Çã‚ÇÅ + i‚Çú * ~C‚Çú`             |
| Output Gate  | What to output         | Sigmoid    | `o‚Çú = œÉ(h‚Çú‚Çã‚ÇÅ W‚Çï‚Çí + x‚Çú W‚Çì‚Çí + b‚Çí)`        |
| Final Output | Class prediction       | Softmax    | `y‚Çú = softmax(W ¬∑ h‚Çú + b)`              |
- **œÉ (sigmoid):** squashes values between 0 and 1  
- **tanh:** squashes values between -1 and 1  
- **softmax:** used for classification outputs

#### Memory Flow Diagram
- Cell state `C‚Çú` flows horizontally across time steps  
- Gates control how much information flows in or out  
- Hidden state `h‚Çú` is used as output at each step

---
## Example: One-Hot Encoding with LSTM

Let‚Äôs consider the word ‚Äúhelp‚Äù and encode each character:

| Character | One-Hot Vector |
|-----------|----------------|
| h         | [1, 0, 0, 0]   |
| e         | [0, 1, 0, 0]   |
| l         | [0, 0, 1, 0]   |
| p         | [0, 0, 0, 1]   |

Assume:

- `x‚Çú = [1, 0, 0, 0]`
- `h‚Çú‚Çã‚ÇÅ = [0, 0]`
- `C‚Çú‚Çã‚ÇÅ = [0, 0]`

Suppose the gates output:

- `f‚Çú = [0.8, 0.3]`
- `i‚Çú = [0.5, 0.7]`
- `CÃÉ‚Çú = [0.2, -0.1]`
- Updated `C‚Çú = [0.1, -0.07]`
- `o‚Çú = [0.9, 0.6]`
- Final `h‚Çú = [0.089, -0.041]`

## Prediction: Using Softmax

To predict the next character:


$$
Y‚Çú = \text{softmax}(W \cdot h‚Çú + b)
$$



This returns the probability of each character (e.g., `h`, `e`, `l`, `p`) being the next one.

If **context window = 3**, and the input is ‚Äúhel‚Äù, LSTM predicts ‚Äúp‚Äù.

## Loss Calculation and Training

LSTM is trained using backpropagation through time (BPTT). The model computes gradients and updates weights for:

- Forget gate: `‚àÇL/‚àÇW_f`
- Input gate: `‚àÇL/‚àÇW_i
- Candidate memory: `‚àÇL/‚àÇW_c`
- Output gate: `‚àÇL/‚àÇW_o`

This allows each gate to be trained **independently for its specific function**, resulting in more effective learning of sequence patterns.

## Summary Table

| Feature                 | RNN                 | LSTM                          |
| ----------------------- | ------------------- | ----------------------------- |
| Memory Type             | Short-term          | Long-term (via cell state)    |
| Handles Long Sequences? | Limited             | Yes                           |
| Architecture            | Simple              | Complex (uses gates)          |
| Problem Solved          | Sequence modeling   | Vanishing gradient            |
| Applications            | Basic sequence data | Language, speech, forecasting |

